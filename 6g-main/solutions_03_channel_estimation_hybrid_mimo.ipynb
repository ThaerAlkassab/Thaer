{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/aldebaro/ai6g/blob/main/solutions_03_channel_estimation_hybrid_mimo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446e0bb9",
   "metadata": {},
   "source": [
    "**Inteligência Artificial e Aprendizado de Máquina Aplicados a Redes 5G e 6G**.\n",
    "*Aldebaro Klautau* (UFPA). Minicurso 5 do SBrT - 25 de setembro de 2022."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install and import all the Needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "# Clone the repository if running in Colab and install all the dependencies\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')\n",
    "    import sys\n",
    "    import os\n",
    "    try:\n",
    "      !git clone https://github.com/aldebaro/ai6g.git\n",
    "    except:\n",
    "      print(\"ai6g is already in the contents\")\n",
    "    %cd ai6g\n",
    "    !ln -s /content/drive/MyDrive/ai6g_files/files_03_channel/* ./files_03_channel\n",
    "    !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install tensorflow==2.10.0 keras-tuner==1.1.3 numpy h5py==3.7.0\n",
    "import h5py \n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense,\n",
    "    Reshape,\n",
    "    Conv2D,\n",
    "    Dropout,\n",
    "    Flatten,\n",
    "    BatchNormalization,\n",
    ")\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    ReduceLROnPlateau,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FILES={\n",
    "    1:\"./files_03_channel/Pilots_SNR_(-5dB)_64x16(80pilots)_single_sub.hdf5\",\n",
    "    2:\"./files_03_channel/Pilots_SNR_(-5dB)_8x4(80pilots)_single_sub.hdf5\",\n",
    "    3:\"./files_03_channel/Pilots_SNR_(0dB)_8x4(80pilots)_single_sub.hdf5\",\n",
    "    \"64x16\":\"./files_03_channel/Channels_64x16_single_sub.hdf5\",\n",
    "    \"8x4\":\"./files_03_channel/Channels_8x4_single_sub.hdf5\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the following values according to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHOSEN_DATASET = 2\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 1000\n",
    "\n",
    "\n",
    "# Change these values acording to the dataset\n",
    "NR = 8 \n",
    "NT = 4 \n",
    "LR = 4 \n",
    "CHANNELS_USED_TRAIN = 9000\n",
    "SUBCARIERS_USED = 1\n",
    "TOTAL_CHANNELS = 10000\n",
    "NUM_SYM_PILOTS = 80\n",
    "CHANNELS_USED_TEST = TOTAL_CHANNELS - CHANNELS_USED_TRAIN\n",
    "\n",
    "\n",
    "\n",
    "input_shape = (NUM_SYM_PILOTS * LR, SUBCARIERS_USED)\n",
    "output_shape = (NR, NT, SUBCARIERS_USED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SELECTING THE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CHOSEN_DATASET == 1:\n",
    "    CHOSEN_CHANNELS = FILES[\"64x16\"]\n",
    "elif CHOSEN_DATASET > 1 and CHOSEN_DATASET <=3:\n",
    "    CHOSEN_CHANNELS = FILES[\"8x4\"]\n",
    "\n",
    "\n",
    "# Directory of the Channels Matrix file\n",
    "CHANNELS_FILE = h5py.File(CHOSEN_CHANNELS, \"r\")\n",
    "\n",
    "\n",
    "# Directory of the Pilots Matrix file\n",
    "PILOTS_FILE = h5py.File(FILES[CHOSEN_DATASET], \"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will be used as a metric in keras to calculate the NMSE(Normalized Mean Squared Error) for each train or validation example that will be iterated in the training process, providing clear metrics to the whole process and enabling the use of early_stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring the function to calculate NMSE for each batch\n",
    "def NMSEtrainComplex(y_true, y_pred):\n",
    "    sub = y_pred[:, :, :] - y_true[:, :, :]\n",
    "    H_k = y_true[:, :, :]\n",
    "    nmse = tf.norm(sub, ord=\"fro\", axis=(1, 2)) ** 2\n",
    "    den = tf.norm(H_k, ord=\"fro\", axis=(1, 2)) ** 2\n",
    "\n",
    "    result = (nmse / den)\n",
    "\n",
    "    return 10*tf.experimental.numpy.log10(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the NN model will be crated, a chain of Dense layers is being used with relu activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=input_shape))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense((512), activation=\"relu\"))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense((512), activation=\"relu\"))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense((512), activation=\"relu\"))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense((512), activation=\"relu\"))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense((512), activation=\"relu\"))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(np.prod(output_shape), activation=\"linear\"))\n",
    "model.add(Reshape(output_shape))\n",
    "model.compile(\n",
    "    loss=\"mean_squared_error\", optimizer=\"adam\", metrics=[NMSEtrainComplex]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "225/225 [==============================] - 12s 17ms/step - loss: 0.6909 - NMSEtrainComplex: -0.0017 - val_loss: 0.3561 - val_NMSEtrainComplex: -2.7802 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "225/225 [==============================] - 3s 13ms/step - loss: 0.3860 - NMSEtrainComplex: -2.3424 - val_loss: 0.2729 - val_NMSEtrainComplex: -4.1251 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "225/225 [==============================] - 3s 13ms/step - loss: 0.3010 - NMSEtrainComplex: -3.4995 - val_loss: 0.2304 - val_NMSEtrainComplex: -5.0783 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "225/225 [==============================] - 4s 16ms/step - loss: 0.2526 - NMSEtrainComplex: -4.3537 - val_loss: 0.2007 - val_NMSEtrainComplex: -5.7514 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.2207 - NMSEtrainComplex: -4.9892 - val_loss: 0.1848 - val_NMSEtrainComplex: -6.1100 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "225/225 [==============================] - 4s 16ms/step - loss: 0.2000 - NMSEtrainComplex: -5.4259 - val_loss: 0.1715 - val_NMSEtrainComplex: -6.4580 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.1862 - NMSEtrainComplex: -5.7150 - val_loss: 0.1635 - val_NMSEtrainComplex: -6.6310 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.1725 - NMSEtrainComplex: -6.0295 - val_loss: 0.1543 - val_NMSEtrainComplex: -6.8938 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "225/225 [==============================] - 4s 16ms/step - loss: 0.1628 - NMSEtrainComplex: -6.2761 - val_loss: 0.1482 - val_NMSEtrainComplex: -7.0781 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "225/225 [==============================] - 3s 16ms/step - loss: 0.1548 - NMSEtrainComplex: -6.4651 - val_loss: 0.1487 - val_NMSEtrainComplex: -7.0925 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.1477 - NMSEtrainComplex: -6.6442 - val_loss: 0.1414 - val_NMSEtrainComplex: -7.2352 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.1440 - NMSEtrainComplex: -6.7302 - val_loss: 0.1404 - val_NMSEtrainComplex: -7.2662 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.1390 - NMSEtrainComplex: -6.8913 - val_loss: 0.1349 - val_NMSEtrainComplex: -7.4638 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.1348 - NMSEtrainComplex: -7.0074 - val_loss: 0.1328 - val_NMSEtrainComplex: -7.4970 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.1296 - NMSEtrainComplex: -7.1742 - val_loss: 0.1322 - val_NMSEtrainComplex: -7.5390 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "225/225 [==============================] - 3s 16ms/step - loss: 0.1278 - NMSEtrainComplex: -7.2414 - val_loss: 0.1312 - val_NMSEtrainComplex: -7.6259 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.1257 - NMSEtrainComplex: -7.3031 - val_loss: 0.1311 - val_NMSEtrainComplex: -7.6088 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "225/225 [==============================] - 4s 16ms/step - loss: 0.1227 - NMSEtrainComplex: -7.4067 - val_loss: 0.1287 - val_NMSEtrainComplex: -7.6965 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.1201 - NMSEtrainComplex: -7.4821 - val_loss: 0.1271 - val_NMSEtrainComplex: -7.7429 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.1191 - NMSEtrainComplex: -7.5092 - val_loss: 0.1283 - val_NMSEtrainComplex: -7.6739 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.1167 - NMSEtrainComplex: -7.6094 - val_loss: 0.1285 - val_NMSEtrainComplex: -7.6577 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.1160 - NMSEtrainComplex: -7.6324 - val_loss: 0.1256 - val_NMSEtrainComplex: -7.8009 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.1111 - NMSEtrainComplex: -7.8195 - val_loss: 0.1245 - val_NMSEtrainComplex: -7.8655 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.1111 - NMSEtrainComplex: -7.8067 - val_loss: 0.1260 - val_NMSEtrainComplex: -7.7884 - lr: 0.0010\n",
      "Epoch 25/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.1100 - NMSEtrainComplex: -7.8594 - val_loss: 0.1222 - val_NMSEtrainComplex: -7.9369 - lr: 0.0010\n",
      "Epoch 26/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.1087 - NMSEtrainComplex: -7.8876 - val_loss: 0.1223 - val_NMSEtrainComplex: -7.9005 - lr: 0.0010\n",
      "Epoch 27/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.1066 - NMSEtrainComplex: -7.9791 - val_loss: 0.1231 - val_NMSEtrainComplex: -7.9012 - lr: 0.0010\n",
      "Epoch 28/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.1065 - NMSEtrainComplex: -7.9816 - val_loss: 0.1239 - val_NMSEtrainComplex: -7.8698 - lr: 0.0010\n",
      "Epoch 29/1000\n",
      "225/225 [==============================] - ETA: 0s - loss: 0.1059 - NMSEtrainComplex: -8.0141\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.1059 - NMSEtrainComplex: -8.0141 - val_loss: 0.1240 - val_NMSEtrainComplex: -7.8595 - lr: 0.0010\n",
      "Epoch 30/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.0976 - NMSEtrainComplex: -8.3804 - val_loss: 0.1149 - val_NMSEtrainComplex: -8.2954 - lr: 5.0000e-04\n",
      "Epoch 31/1000\n",
      "225/225 [==============================] - 4s 16ms/step - loss: 0.0942 - NMSEtrainComplex: -8.5089 - val_loss: 0.1163 - val_NMSEtrainComplex: -8.2942 - lr: 5.0000e-04\n",
      "Epoch 32/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.0923 - NMSEtrainComplex: -8.5835 - val_loss: 0.1143 - val_NMSEtrainComplex: -8.3294 - lr: 5.0000e-04\n",
      "Epoch 33/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.0903 - NMSEtrainComplex: -8.6938 - val_loss: 0.1130 - val_NMSEtrainComplex: -8.3851 - lr: 5.0000e-04\n",
      "Epoch 34/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.0901 - NMSEtrainComplex: -8.6983 - val_loss: 0.1146 - val_NMSEtrainComplex: -8.3438 - lr: 5.0000e-04\n",
      "Epoch 35/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.0894 - NMSEtrainComplex: -8.7151 - val_loss: 0.1126 - val_NMSEtrainComplex: -8.4175 - lr: 5.0000e-04\n",
      "Epoch 36/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.0893 - NMSEtrainComplex: -8.7333 - val_loss: 0.1148 - val_NMSEtrainComplex: -8.3192 - lr: 5.0000e-04\n",
      "Epoch 37/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.0887 - NMSEtrainComplex: -8.7535 - val_loss: 0.1131 - val_NMSEtrainComplex: -8.3843 - lr: 5.0000e-04\n",
      "Epoch 38/1000\n",
      "225/225 [==============================] - 4s 16ms/step - loss: 0.0887 - NMSEtrainComplex: -8.7736 - val_loss: 0.1122 - val_NMSEtrainComplex: -8.4080 - lr: 5.0000e-04\n",
      "Epoch 39/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.0881 - NMSEtrainComplex: -8.7827 - val_loss: 0.1143 - val_NMSEtrainComplex: -8.3314 - lr: 5.0000e-04\n",
      "Epoch 40/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.0885 - NMSEtrainComplex: -8.7558 - val_loss: 0.1152 - val_NMSEtrainComplex: -8.3573 - lr: 5.0000e-04\n",
      "Epoch 41/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.0880 - NMSEtrainComplex: -8.7921 - val_loss: 0.1146 - val_NMSEtrainComplex: -8.3662 - lr: 5.0000e-04\n",
      "Epoch 42/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.0872 - NMSEtrainComplex: -8.8201 - val_loss: 0.1136 - val_NMSEtrainComplex: -8.3827 - lr: 5.0000e-04\n",
      "Epoch 43/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.0865 - NMSEtrainComplex: -8.8721 - val_loss: 0.1138 - val_NMSEtrainComplex: -8.4243 - lr: 5.0000e-04\n",
      "Epoch 44/1000\n",
      "225/225 [==============================] - 3s 16ms/step - loss: 0.0862 - NMSEtrainComplex: -8.8685 - val_loss: 0.1143 - val_NMSEtrainComplex: -8.4094 - lr: 5.0000e-04\n",
      "Epoch 45/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.0845 - NMSEtrainComplex: -8.9601 - val_loss: 0.1147 - val_NMSEtrainComplex: -8.3932 - lr: 5.0000e-04\n",
      "Epoch 46/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 4s 16ms/step - loss: 0.0841 - NMSEtrainComplex: -8.9700 - val_loss: 0.1143 - val_NMSEtrainComplex: -8.3340 - lr: 5.0000e-04\n",
      "Epoch 47/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.0849 - NMSEtrainComplex: -8.9255 - val_loss: 0.1139 - val_NMSEtrainComplex: -8.3882 - lr: 5.0000e-04\n",
      "Epoch 48/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.0845 - NMSEtrainComplex: -8.9569 - val_loss: 0.1152 - val_NMSEtrainComplex: -8.3271 - lr: 5.0000e-04\n",
      "Epoch 49/1000\n",
      "225/225 [==============================] - 3s 16ms/step - loss: 0.0843 - NMSEtrainComplex: -8.9562 - val_loss: 0.1147 - val_NMSEtrainComplex: -8.3270 - lr: 5.0000e-04\n",
      "Epoch 50/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.0832 - NMSEtrainComplex: -9.0141 - val_loss: 0.1144 - val_NMSEtrainComplex: -8.3750 - lr: 5.0000e-04\n",
      "Epoch 51/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.0822 - NMSEtrainComplex: -9.0575 - val_loss: 0.1159 - val_NMSEtrainComplex: -8.2992 - lr: 5.0000e-04\n",
      "Epoch 52/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.0829 - NMSEtrainComplex: -9.0379 - val_loss: 0.1135 - val_NMSEtrainComplex: -8.3984 - lr: 5.0000e-04\n",
      "Epoch 53/1000\n",
      "222/225 [============================>.] - ETA: 0s - loss: 0.0825 - NMSEtrainComplex: -9.0284\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.0824 - NMSEtrainComplex: -9.0358 - val_loss: 0.1132 - val_NMSEtrainComplex: -8.4360 - lr: 5.0000e-04\n",
      "Epoch 54/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.0784 - NMSEtrainComplex: -9.2746 - val_loss: 0.1109 - val_NMSEtrainComplex: -8.5616 - lr: 2.5000e-04\n",
      "Epoch 55/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.0781 - NMSEtrainComplex: -9.2723 - val_loss: 0.1117 - val_NMSEtrainComplex: -8.5124 - lr: 2.5000e-04\n",
      "Epoch 56/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.0770 - NMSEtrainComplex: -9.3286 - val_loss: 0.1104 - val_NMSEtrainComplex: -8.5598 - lr: 2.5000e-04\n",
      "Epoch 57/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.0764 - NMSEtrainComplex: -9.3696 - val_loss: 0.1109 - val_NMSEtrainComplex: -8.5416 - lr: 2.5000e-04\n",
      "Epoch 58/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.0772 - NMSEtrainComplex: -9.3019 - val_loss: 0.1115 - val_NMSEtrainComplex: -8.5165 - lr: 2.5000e-04\n",
      "Epoch 59/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.0756 - NMSEtrainComplex: -9.4103 - val_loss: 0.1100 - val_NMSEtrainComplex: -8.5991 - lr: 2.5000e-04\n",
      "Epoch 60/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.0764 - NMSEtrainComplex: -9.3544 - val_loss: 0.1093 - val_NMSEtrainComplex: -8.6177 - lr: 2.5000e-04\n",
      "Epoch 61/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.0762 - NMSEtrainComplex: -9.3675 - val_loss: 0.1118 - val_NMSEtrainComplex: -8.5006 - lr: 2.5000e-04\n",
      "Epoch 62/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.0749 - NMSEtrainComplex: -9.4539 - val_loss: 0.1106 - val_NMSEtrainComplex: -8.5556 - lr: 2.5000e-04\n",
      "Epoch 63/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.0747 - NMSEtrainComplex: -9.4554 - val_loss: 0.1107 - val_NMSEtrainComplex: -8.5717 - lr: 2.5000e-04\n",
      "Epoch 64/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.0758 - NMSEtrainComplex: -9.3870 - val_loss: 0.1098 - val_NMSEtrainComplex: -8.6069 - lr: 2.5000e-04\n",
      "Epoch 65/1000\n",
      "225/225 [==============================] - 3s 13ms/step - loss: 0.0759 - NMSEtrainComplex: -9.3782 - val_loss: 0.1109 - val_NMSEtrainComplex: -8.5443 - lr: 2.5000e-04\n",
      "Epoch 66/1000\n",
      "225/225 [==============================] - 3s 13ms/step - loss: 0.0753 - NMSEtrainComplex: -9.4202 - val_loss: 0.1116 - val_NMSEtrainComplex: -8.4943 - lr: 2.5000e-04\n",
      "Epoch 67/1000\n",
      "225/225 [==============================] - 3s 13ms/step - loss: 0.0745 - NMSEtrainComplex: -9.4428 - val_loss: 0.1110 - val_NMSEtrainComplex: -8.5350 - lr: 2.5000e-04\n",
      "Epoch 68/1000\n",
      "225/225 [==============================] - 3s 13ms/step - loss: 0.0754 - NMSEtrainComplex: -9.4133 - val_loss: 0.1120 - val_NMSEtrainComplex: -8.4852 - lr: 2.5000e-04\n",
      "Epoch 69/1000\n",
      "225/225 [==============================] - 3s 13ms/step - loss: 0.0750 - NMSEtrainComplex: -9.4257 - val_loss: 0.1122 - val_NMSEtrainComplex: -8.4730 - lr: 2.5000e-04\n",
      "Epoch 70/1000\n",
      "225/225 [==============================] - 3s 13ms/step - loss: 0.0740 - NMSEtrainComplex: -9.4804 - val_loss: 0.1107 - val_NMSEtrainComplex: -8.5549 - lr: 2.5000e-04\n",
      "Epoch 71/1000\n",
      "225/225 [==============================] - 3s 13ms/step - loss: 0.0737 - NMSEtrainComplex: -9.5127 - val_loss: 0.1115 - val_NMSEtrainComplex: -8.5102 - lr: 2.5000e-04\n",
      "Epoch 72/1000\n",
      "225/225 [==============================] - 3s 13ms/step - loss: 0.0745 - NMSEtrainComplex: -9.4490 - val_loss: 0.1111 - val_NMSEtrainComplex: -8.5270 - lr: 2.5000e-04\n",
      "Epoch 73/1000\n",
      "225/225 [==============================] - 4s 16ms/step - loss: 0.0731 - NMSEtrainComplex: -9.5461 - val_loss: 0.1100 - val_NMSEtrainComplex: -8.6169 - lr: 2.5000e-04\n",
      "Epoch 74/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.0739 - NMSEtrainComplex: -9.4935 - val_loss: 0.1115 - val_NMSEtrainComplex: -8.5150 - lr: 2.5000e-04\n",
      "Epoch 75/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.0735 - NMSEtrainComplex: -9.5069 - val_loss: 0.1105 - val_NMSEtrainComplex: -8.5572 - lr: 2.5000e-04\n",
      "Epoch 76/1000\n",
      "225/225 [==============================] - 3s 16ms/step - loss: 0.0733 - NMSEtrainComplex: -9.5404 - val_loss: 0.1118 - val_NMSEtrainComplex: -8.4846 - lr: 2.5000e-04\n",
      "Epoch 77/1000\n",
      "225/225 [==============================] - ETA: 0s - loss: 0.0731 - NMSEtrainComplex: -9.5300\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.0731 - NMSEtrainComplex: -9.5300 - val_loss: 0.1112 - val_NMSEtrainComplex: -8.5088 - lr: 2.5000e-04\n",
      "Epoch 78/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.0729 - NMSEtrainComplex: -9.5516 - val_loss: 0.1108 - val_NMSEtrainComplex: -8.5665 - lr: 1.2500e-04\n",
      "Epoch 79/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.0724 - NMSEtrainComplex: -9.5653 - val_loss: 0.1095 - val_NMSEtrainComplex: -8.6275 - lr: 1.2500e-04\n",
      "Epoch 80/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.0713 - NMSEtrainComplex: -9.6363 - val_loss: 0.1092 - val_NMSEtrainComplex: -8.6624 - lr: 1.2500e-04\n",
      "Epoch 81/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.0716 - NMSEtrainComplex: -9.6228 - val_loss: 0.1094 - val_NMSEtrainComplex: -8.6414 - lr: 1.2500e-04\n",
      "Epoch 82/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.0711 - NMSEtrainComplex: -9.6570 - val_loss: 0.1096 - val_NMSEtrainComplex: -8.6272 - lr: 1.2500e-04\n",
      "Epoch 83/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.0700 - NMSEtrainComplex: -9.7356 - val_loss: 0.1090 - val_NMSEtrainComplex: -8.6582 - lr: 1.2500e-04\n",
      "Epoch 84/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.0706 - NMSEtrainComplex: -9.6818 - val_loss: 0.1101 - val_NMSEtrainComplex: -8.5924 - lr: 1.2500e-04\n",
      "Epoch 85/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.0709 - NMSEtrainComplex: -9.6521 - val_loss: 0.1098 - val_NMSEtrainComplex: -8.5881 - lr: 1.2500e-04\n",
      "Epoch 86/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.0697 - NMSEtrainComplex: -9.7301 - val_loss: 0.1097 - val_NMSEtrainComplex: -8.6087 - lr: 1.2500e-04\n",
      "Epoch 87/1000\n",
      "225/225 [==============================] - 4s 16ms/step - loss: 0.0701 - NMSEtrainComplex: -9.7121 - val_loss: 0.1098 - val_NMSEtrainComplex: -8.6127 - lr: 1.2500e-04\n",
      "Epoch 88/1000\n",
      "225/225 [==============================] - 3s 15ms/step - loss: 0.0704 - NMSEtrainComplex: -9.6860 - val_loss: 0.1095 - val_NMSEtrainComplex: -8.6398 - lr: 1.2500e-04\n",
      "Epoch 89/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213/225 [===========================>..] - ETA: 0s - loss: 0.0701 - NMSEtrainComplex: -9.7047"
     ]
    }
   ],
   "source": [
    "\n",
    "training_inputData = PILOTS_FILE[\"pilots\"][:CHANNELS_USED_TRAIN, :, :SUBCARIERS_USED]\n",
    "training_outputData = CHANNELS_FILE[\"channels\"][:CHANNELS_USED_TRAIN, :, :, :SUBCARIERS_USED]\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "        training_inputData,\n",
    "        training_outputData,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        verbose=1,\n",
    "        shuffle=True,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[\n",
    "            EarlyStopping(\n",
    "                monitor=\"val_NMSEtrainComplex\",\n",
    "                min_delta=5e-3,\n",
    "                patience=40,\n",
    "                restore_best_weights=True,\n",
    "            ),\n",
    "            ReduceLROnPlateau(\n",
    "                factor=0.5,\n",
    "                min_delta=5e-2,\n",
    "                patience=20,\n",
    "                cooldown=5,\n",
    "                verbose=1,\n",
    "                min_lr=1e-7,\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "model.save(\"./files_03_channel/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlModel = load_model(\"./files_03_channel/model\", custom_objects={\"NMSEtrainComplex\": NMSEtrainComplex})\n",
    "\n",
    "TEST_INPUT = PILOTS_FILE[\"pilots\"][CHANNELS_USED_TRAIN:, :, :SUBCARIERS_USED]\n",
    "TEST_OUTPUT = CHANNELS_FILE[\"channels\"][CHANNELS_USED_TRAIN:, :, :, :SUBCARIERS_USED]\n",
    "\n",
    "results = mlModel.evaluate(TEST_INPUT,TEST_OUTPUT)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e85753",
   "metadata": {},
   "source": [
    "## Authors\n",
    "\n",
    "- Prof. Francisco Muller - LASSE/UFPA\n",
    "- Daniel Oliveira - LASSE/UFPA\n",
    "- Claudio Mello - LASSE/UFPA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('3.8.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "5aaf80bf5bb851baa857fc3e2a05055a4723060c18483edce1107b583200cb68"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
