{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/aldebaro/ai6g/blob/main/08_finite_mdp_reinforcement_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc42cc4f",
   "metadata": {},
   "source": [
    "**Inteligência Artificial e Aprendizado de Máquina Aplicados a Redes 5G e 6G**.\n",
    "*Aldebaro Klautau* (UFPA). Minicurso 5 do SBrT - 25 de setembro de 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository if running in Colab and install all the dependencies\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')\n",
    "    import sys\n",
    "    import os\n",
    "    try:\n",
    "      !git clone https://github.com/aldebaro/ai6g.git\n",
    "    except:\n",
    "      print(\"ai6g is already in the contents\")\n",
    "    %cd ai6g\n",
    "    !ln -s /content/drive/MyDrive/ai6g_files/files_08_mdp/* ./files_08_mdp\n",
    "    !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inteligência Artificial e Aprendizado de Máquina Aplicados a Redes 5G e 6G**. *Aldebaro Klautau* (UFPA). Minicurso 5 do SBrT - 25 de setembro de 2022."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Scheduling scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In the User Scheduling scenario, the state is defined as the position of the two users and their buffers occupancy and the action is to select one of the two users.\n",
    "Given that Nu=2 and G=6, there are S= (G²)*(G²)*(B+1)*Nu states, where G is the grid dimension, Nu is the number of users and B is the buffer size, and A = 2 actions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd files_08_mdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import sys\n",
    "from src.FiniteMDP import FiniteMDP\n",
    "from numpy.random import randint\n",
    "from src.createData import *\n",
    "from src.NextStateProbabilitiesEnv import NextStateProbabilitiesEnv\n",
    "\n",
    "\n",
    "class UserSchedulingEnv(NextStateProbabilitiesEnv):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.ues_pos_prob, self.channel_spectral_efficiencies, self.ues_valid_actions = self.read_external_files()\n",
    "        self.actions_move = np.array([\n",
    "            [-1, 0],\n",
    "            [0, 1],\n",
    "            [1, 0],\n",
    "            [0, -1],\n",
    "            [0, 0],\n",
    "        ])  # Up, right, down, left, stay\n",
    "        self.Nu = 2\n",
    "        nextStateProbability, rewardsTable = self.createEnvironment()\n",
    "        super().__init__(nextStateProbability, rewardsTable)\n",
    "\n",
    "\n",
    "    def read_external_files(self):\n",
    "        ue0_file = np.load(\"./mobility_ue0.npz\")\n",
    "        ue0 = ue0_file.f.matrix_pos_prob\n",
    "        ue0_valid = ue0_file.f.pos_actions_prob\n",
    "        ue1_file = np.load(\"./mobility_ue1.npz\")\n",
    "        ue1 = ue1_file.f.matrix_pos_prob\n",
    "        ue1_valid = ue1_file.f.pos_actions_prob\n",
    "        capacity = np.load(\"./spec_eff_matrix.npz\")\n",
    "        capacity = capacity.f.spec_eff_matrix\n",
    "\n",
    "\n",
    "        return ([ue0,ue1],capacity, [ue0_valid,ue1_valid])\n",
    "\n",
    "    def createEnvironment(self):\n",
    "    \n",
    "        G = 6 #grid dimension\n",
    "        B = 3  # buffer size\n",
    "        Nu = 2 #number of users\n",
    "        num_incoming_packets_per_time_slot = 2\n",
    "\n",
    "        print(\" States = \", (G**2)*(G**2)*((B+1)**Nu))\n",
    "\n",
    "        indexGivenActionDictionary, actionGivenIndexList = createActionsDataStructures()\n",
    "        A = len(actionGivenIndexList)\n",
    "\n",
    "        indexGivenStateDictionary, stateGivenIndexList = createStatesDataStructures()\n",
    "        S = len(stateGivenIndexList)\n",
    "\n",
    "        nextStateProbability = np.zeros((S, A, S))\n",
    "        rewardsTable = np.zeros((S, A, S))\n",
    "        for s in range(S):\n",
    "            #current state:\n",
    "            currentState = stateGivenIndexList[s]\n",
    "            #interpret the state\n",
    "            (all_positions, buffers_occupancy) = currentState \n",
    "            for a in range(A):                \n",
    "                currentAction = actionGivenIndexList[a]\n",
    "                #in this case, the action is the user\n",
    "                chosen_user = a \n",
    "                #get the channels spectral efficiency (SE)\n",
    "                chosen_user_position = all_positions[chosen_user]\n",
    "                #transmitted packets \n",
    "                se = self.channel_spectral_efficiencies[chosen_user_position[0],chosen_user_position[1]]\n",
    "                #transmitted packets \n",
    "                transmitRate = se \n",
    "\n",
    "                new_buffer = np.array(buffers_occupancy)\n",
    "                #decrement buffer of chosen user\n",
    "                new_buffer[chosen_user] -= transmitRate \n",
    "                new_buffer[new_buffer<0] = 0\n",
    "                #arrival of new packets\n",
    "                new_buffer += num_incoming_packets_per_time_slot \n",
    "\n",
    "                #check if overflow\n",
    "                #in case positive, limit the buffers to maximum capacity\n",
    "                number_dropped_packets = new_buffer - B\n",
    "                number_dropped_packets[number_dropped_packets<0] = 0\n",
    "\n",
    "                #saturate buffer levels\n",
    "                new_buffer -= number_dropped_packets\n",
    "\n",
    "                buffers_occupancy=tuple(new_buffer) #convert to tuple to compose state\n",
    "\n",
    "                # calculate rewards\n",
    "                sumDrops = np.sum(number_dropped_packets)\n",
    "                r = -sumDrops\n",
    "\n",
    "                for ue1_action in np.arange(5):\n",
    "                    for ue2_action in np.arange(5):\n",
    "                        prob_ue1_action = self.ues_valid_actions[0][all_positions[0][0], all_positions[0][1]][ue1_action]\n",
    "                        prob_ue2_action = self.ues_valid_actions[1][all_positions[1][0], all_positions[1][1]][ue2_action]\n",
    "                        if prob_ue1_action!=0 and prob_ue2_action!=0:\n",
    "                            #calculate nextState\n",
    "                            new_position_ue1 = np.array(all_positions[0]) + self.actions_move[ue1_action]\n",
    "                            new_position_ue2 = np.array(all_positions[1]) + self.actions_move[ue2_action]\n",
    "                            new_position = ((new_position_ue1[0],new_position_ue1[1]), (new_position_ue2[0],new_position_ue2[1]))\n",
    "                            nextState = (new_position, buffers_occupancy)\n",
    "\n",
    "                            # probabilistic part: consider the user mobility\n",
    "                            nextStateIndice = indexGivenStateDictionary[nextState]\n",
    "                            #take in account mobility\n",
    "                            nextStateProbability[s, a, nextStateIndice] = prob_ue1_action * prob_ue2_action\n",
    "                            rewardsTable[s, a, nextStateIndice] = r\n",
    "        self.indexGivenActionDictionary = indexGivenActionDictionary\n",
    "        self.actionGivenIndexList = actionGivenIndexList\n",
    "        self.indexGivenStateDictionary = indexGivenStateDictionary\n",
    "        self.stateGivenIndexList = stateGivenIndexList\n",
    "\n",
    "        return nextStateProbability, rewardsTable\n",
    "\n",
    "       \n",
    "    def enable_rendering(self):\n",
    "        self.should_render = True\n",
    "        from render_user_scheduling import Scheduling_RL_render\n",
    "        self.schedule_render =  Scheduling_RL_render()\n",
    "    \n",
    "    def render(self):\n",
    "        self.schedule_render.render()\n",
    "\n",
    "    def close (self):\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the state of the environment and returns an initial observation.\n",
    "        Returns\n",
    "        -------\n",
    "        observation (object): the initial observation of the space.\n",
    "        \"\"\"\n",
    "        show_debug_info = True\n",
    "        \n",
    "        indexGivenStateDictionary, stateGivenIndexList = createStatesDataStructures()\n",
    "        self.currentIteration = 0\n",
    "        self.episode_return = 0\n",
    "\n",
    "        #In the beginning, choose user actions randomly\n",
    "        a = randint(0,4,size=(self.Nu,))\n",
    "        self.actions_move = a\n",
    "\n",
    "        total_states = len(stateGivenIndexList)\n",
    "\n",
    "        #Chose a state out of all the possible ones\n",
    "        s = randint(0, total_states)\n",
    "        self.current_state_index = stateGivenIndexList[s]\n",
    "       \n",
    "        return total_states\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    env = UserSchedulingEnv()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman Algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from src.FiniteMDP import FiniteMDP\n",
    "\n",
    "episodes = np.arange(900, 1000)\n",
    "n_steps = 1000\n",
    "se = np.load(\"./spec_eff_matrix.npz\")\n",
    "se = se.f.spec_eff_matrix\n",
    "env = UserSchedulingEnv()\n",
    "file_states_actions = np.load(\"./states_actions.npz\", allow_pickle=True)\n",
    "indexGivenStateDictionary = file_states_actions.f.indexGivenStateDictionary.item()\n",
    "file_optimal_values = np.load(\"./optimal_values.npz\")\n",
    "optimal_policy = file_optimal_values.f.optimal_policy\n",
    "mdp = FiniteMDP(env)\n",
    "\n",
    "rewards = np.zeros((len(episodes), n_steps))\n",
    "n_users = 2\n",
    "\n",
    "\n",
    "buffer_size = 3\n",
    "num_incoming_packets_per_time_slot = 2\n",
    "rewards = np.zeros((len(episodes), n_steps))\n",
    "for n_episode, episode in enumerate(episodes):\n",
    "\tfile_pos = np.load(\"./mobility_traces/ep{}.npz\".format(episode))\n",
    "\tfor step in np.arange(n_steps):\n",
    "\t\tprint(\"Episode {}, step {}\".format(n_episode, step))\n",
    "\t\tpos_ues = ((file_pos.f.ue1[step][0], file_pos.f.ue1[step][1]), (file_pos.f.ue2[step][0], file_pos.f.ue2[step][1]))\n",
    "\t\tif step == 0:\n",
    "\t\t\tbuffers = np.array([0, 0])\n",
    "\t\tstate = (pos_ues, tuple(buffers))\n",
    "\t\tprob_actions = optimal_policy[indexGivenStateDictionary[state]]\n",
    "\t\t\n",
    "\t\t#in this case, the action is the user\n",
    "\t\tchosen_user = np.random.choice(2, p=prob_actions) \n",
    "\t\tnumber_dropped_packets = 0\n",
    "\t\tfor user in np.arange(n_users):\n",
    "\t\t\tif user == chosen_user:\n",
    "\t\t\t\t#get the channels spectral efficiency (SE)\n",
    "\t\t\t\tchosen_user_position = pos_ues[user]\n",
    "\n",
    "\t\t\t\tse_chosen_ue = se[int(chosen_user_position[0]),int(chosen_user_position[1])]\n",
    "\t\t\t\t#based on selected (chosen) user, update its buffer\n",
    "\t\t\t\ttransmitRate = se_chosen_ue #transmitted packets \n",
    "\t\t\t\tbuffers[chosen_user] -= transmitRate #decrement buffer of chosen user\n",
    "\t\t\t\tbuffers[buffers<0] = 0\n",
    "\t\t\tbuffers[user] += num_incoming_packets_per_time_slot #arrival of new packets\n",
    "\n",
    "\t\t\t#check if overflow\n",
    "\t\t\t#in case positive, limit the buffers to maximum capacity\n",
    "\t\t\tnumber_dropped_packets = buffers[user] - buffer_size\n",
    "\t\t\tnumber_dropped_packets = 0 if number_dropped_packets < 0 else number_dropped_packets\n",
    "\n",
    "\t\t\t#saturate buffer levels\n",
    "\t\t\tbuffers[user] = buffer_size if buffers[user]>buffer_size else buffers[user]\n",
    "\n",
    "\t\t\t# calculate rewards\n",
    "\t\t\trewards[n_episode, step] -= number_dropped_packets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integer Programming Algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IP_algorithm( n_steps, should_render=False):\n",
    "\t\n",
    "\n",
    "\tepisodes = np.arange(900, 1000)\n",
    "\tnum_incoming_packets_per_time_slot = 2\n",
    "\tbuffer_size = 3\n",
    "\tn_users = 2\n",
    "\n",
    "\tse = np.load(\"./spec_eff_matrix.npz\")\n",
    "\tse = se.f.spec_eff_matrix\n",
    "\n",
    "\trewards = np.zeros((len(episodes), n_steps))\n",
    "\n",
    "\tactions = np.load(\"./actions_opt.npz\")\n",
    "\tactions = actions.f.actions\n",
    "\n",
    "\n",
    "\trewards = np.zeros((len(episodes), n_steps))\n",
    "\tfor n_episode, episode in enumerate(episodes):\n",
    "\t\tfile_pos = np.load(\"./mobility_traces/ep{}.npz\".format(episode))\n",
    "\t\tfor step in np.arange(n_steps):\n",
    "\t\t\tprint(\"Episode {}, step {}\".format(n_episode, step))\n",
    "\t\t\tpos_ues = ((file_pos.f.ue1[step][0], file_pos.f.ue1[step][1]), (file_pos.f.ue2[step][0], file_pos.f.ue2[step][1]))\n",
    "\t\t\tif step == 0:\n",
    "\t\t\t\tbuffers = np.array([0, 0])\n",
    "\t\t\tstate = (pos_ues, tuple(buffers))\n",
    "\t\t\n",
    "\t\t\tchosen_user = int(actions[n_episode, step])\n",
    "\t\t\tnumber_dropped_packets = 0\n",
    "\t\t\tfor user in np.arange(n_users):\n",
    "\t\t\t\tif user == chosen_user:\n",
    "\n",
    "\t\t\t\t\t#get the channels spectral efficiency (SE)\n",
    "\t\t\t\t\tchosen_user_position = pos_ues[user]\n",
    "\t\t\t\t\t#based on selected (chosen) user, update its buffer\n",
    "\t\t\t\t\tse_chosen_ue = se[int(chosen_user_position[0]),int(chosen_user_position[1])]\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t#transmitted packets \n",
    "\t\t\t\t\ttransmitRate = se_chosen_ue \n",
    "\t\t\t\t\t#decrement buffer of chosen user\n",
    "\t\t\t\t\tbuffers[chosen_user] -= transmitRate \n",
    "\t\t\t\t\tbuffers[buffers<0] = 0\n",
    "\t\t\t\t\t#arrival of new packets\n",
    "\t\t\t\tbuffers[user] += num_incoming_packets_per_time_slot \n",
    "\n",
    "\t\t\t\t#check if overflow\n",
    "\t\t\t\t#in case positive, limit the buffers to maximum capacity\n",
    "\t\t\t\tnumber_dropped_packets = buffers[user] - buffer_size\n",
    "\t\t\t\tnumber_dropped_packets = 0 if number_dropped_packets < 0 else number_dropped_packets\n",
    "\n",
    "\t\t\t\t#saturate buffer levels\n",
    "\t\t\t\tbuffers[user] = buffer_size if buffers[user]>buffer_size else buffers[user]\n",
    "\n",
    "\t\t\t\t# calculate rewards\n",
    "\t\t\t\trewards[n_episode, step] -= number_dropped_packets\n",
    "\n",
    "\t\t\t\tif should_render:\n",
    "\t\t\t\t\tfrom src.render_user_scheduling import Scheduling_RL_render\n",
    "\t\t\t\t\tschedule_render = Scheduling_RL_render()\t\n",
    "\t\t\t\t\tschedule_render.set_positions(pos_ues, chosen_user)\n",
    "\t\t\t\t\tschedule_render.render()\n",
    "\t\n",
    "\t\n",
    "\tenv = UserSchedulingEnv()\n",
    "\tenv.reset()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tIP_algorithm(n_steps=10, should_render = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Integer Programming, Bellman and Deep RL algorithm performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "reward_mdp = np.load(\"./hist/rewards_Bellman.npz\")\n",
    "reward_mdp = reward_mdp.f.rewards\n",
    "reward_sac = np.load(\"./hist/rewards_sac.npz\")\n",
    "reward_sac = reward_sac.f.rewards\n",
    "reward_td3 = np.load(\"./hist/rewards_td3.npz\")\n",
    "reward_td3 = reward_td3.f.rewards\n",
    "reward_ppo = np.load(\"./hist/rewards_ppo.npz\")\n",
    "reward_ppo = reward_ppo.f.rewards\n",
    "reward_opt = np.load(\"./hist/rewards_IP.npz\")\n",
    "reward_opt = reward_opt.f.rewards\n",
    "\n",
    "n_steps = 1000\n",
    "ep_number = 99 # Equivalent to the 999 since we start at 900\n",
    "\n",
    "# Cumulative reward episode 999\n",
    "w, h = plt.figaspect(0.6)\n",
    "fig = plt.figure(figsize=(w, h))\n",
    "plt.xlabel(\"Step (n)\", fontsize=14)\n",
    "plt.ylabel(\"Cumulative reward\", fontsize=14)\n",
    "plt.grid()\n",
    "plt.plot(np.arange(n_steps), np.cumsum(reward_opt[ep_number]), label=\"IP\")\n",
    "plt.plot(np.arange(n_steps), np.cumsum(reward_mdp[ep_number]), label=\"Bellman\")\n",
    "plt.plot(np.arange(n_steps), np.cumsum(reward_sac[ep_number]), label=\"SAC\")\n",
    "plt.plot(np.arange(n_steps), np.cumsum(reward_td3[ep_number]), label=\"TD3\")\n",
    "plt.plot(np.arange(n_steps), np.cumsum(reward_ppo[ep_number]), label=\"PPO\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.xticks(fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Histogram\n",
    "w, h = plt.figaspect(0.6)\n",
    "fig = plt.figure(figsize=(w, h))\n",
    "plt.xlabel(\"Cumulative reward\", fontsize=14)\n",
    "plt.ylabel(\"Frequency\", fontsize=14)\n",
    "plt.grid()\n",
    "plt.hist(np.sum(reward_opt, axis=1), label=\"IP\", alpha=0.5)\n",
    "plt.hist(np.sum(reward_mdp, axis=1), label=\"Bellman\", alpha=0.5)\n",
    "plt.hist(np.sum(reward_sac, axis=1), label=\"SAC\", alpha=0.5)\n",
    "plt.hist(np.sum(reward_td3, axis=1), label=\"TD3\", alpha=0.5)\n",
    "plt.hist(np.sum(reward_ppo, axis=1), label=\"PPO\", alpha=0.5)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.xticks(fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e85753",
   "metadata": {},
   "source": [
    "## Authors\n",
    "\n",
    "- [Rebecca Aben-Athar - LASSE/UFPA](https://github.com/rebeccaathar)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 ('tabular_rl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dae3497a1346d4c4d589b8d71160bf2a68cb0845f842cd27559a2c01aae19ced"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
